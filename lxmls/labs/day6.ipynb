{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 6: Sequence Models in Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6.1 \n",
    "Convince yourself a RNN is just an MLP with inputs and outputs at various layers. Run the NumpyRNN code. Set break-points and compare with what you learned about back-propagation in the previous day.\n",
    "\n",
    "Start by loading data Part-of-speech data and configure it for the exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add tools\n",
    "# NOTE: This should only be needed if you do not store the notebook on the lxmls root\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "from pdb import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Location of Part-of-Speech WSJ Data\n",
    "WSJ_TRAIN = \"../../data/train-02-21.conll\"\n",
    "WSJ_TEST = \"../../data/test-23.conll\"\n",
    "WSJ_DEV = \"../../data/dev-22.conll\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load Part-of-Speech data \n",
    "import lxmls.readers.pos_corpus as pcc\n",
    "corpus = pcc.PostagCorpus()\n",
    "train_seq = corpus.read_sequence_list_conll(WSJ_TRAIN, max_sent_len=15, max_nr_sent=1000)\n",
    "test_seq = corpus.read_sequence_list_conll(WSJ_TEST, max_sent_len=15, max_nr_sent=1000)\n",
    "dev_seq = corpus.read_sequence_list_conll(WSJ_DEV, max_sent_len=15, max_nr_sent=1000) \n",
    "# Redo indices so that they are consecutive. Also cast all data to numpy arrays\n",
    "# of int32 for compatibility with GPUs and theano and add reverse index\n",
    "train_seq, test_seq, dev_seq = pcc.compacify(train_seq, test_seq, dev_seq, theano=True)\n",
    "# Get number of words and tags in the corpus\n",
    "nr_words = len(train_seq.x_dict)\n",
    "nr_tags = len(train_seq.y_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'lxmls.deep_learning.rnn' from '../../lxmls/deep_learning/rnn.pyc'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lxmls.deep_learning.rnn as rnns\n",
    "reload(rnns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# RNN configuration\n",
    "SEED = 1234       # Random seed to initialize weigths\n",
    "emb_size = 50     # Size of word embeddings\n",
    "hidden_size = 20  # size of hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np_rnn = rnns.NumpyRNN(nr_words, emb_size, hidden_size, nr_tags, seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x0 = train_seq[0].x\n",
    "y0 = train_seq[0].y\n",
    "loos, p_y, y_rnn, h, z1, x = np_rnn.forward(x0, all_outputs=True, \n",
    "                                            outputs=y0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute gradients\n",
    "numpy_rnn_gradients = np_rnn.grads(x0, y0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6.2\n",
    "Understand the basics of scan with this examples. Scan allows you to build computation graphs with a variable number of nodes. It acts as a python \"for\" loop but it is symbolic. The following example should help you understand the basic scan functionality. It generates a sequence for a given length. Run it and modify it. Try to arrive at an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "theano.config.optimizer='None'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def square(x): \n",
    "    return x**2 \n",
    "\n",
    "# Python\n",
    "def np_square_n_steps(nr_steps):\n",
    "    out = []\n",
    "    for n in np.arange(nr_steps):\n",
    "        out.append(square(n))\n",
    "    return np.array(out)\n",
    "    \n",
    "# Theano\n",
    "nr_steps = T.lscalar('nr_steps')\n",
    "h, _ = theano.scan(fn=square, sequences=T.arange(nr_steps))\n",
    "th_square_n_steps = theano.function([nr_steps], h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  4  9 16 25 36 49 64 81]\n",
      "[ 0  1  4  9 16 25 36 49 64 81]\n"
     ]
    }
   ],
   "source": [
    "print np_square_n_steps(10)\n",
    "print th_square_n_steps(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following example should help you understand about matrix multiplications and passing values from one iteration to the other. It at each step it we multiply the output of the previous step by a matrix A. We start with an initial vector s0. The matrix and vector are random but normalized to result on a Markov chain.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "nr_states = 3\n",
    "nr_steps = 5\n",
    "\n",
    "# Transition matrix\n",
    "A = np.abs(np.random.randn(nr_states, nr_states))\n",
    "A = A/A.sum(0, keepdims=True)\n",
    "# Initial state\n",
    "s0 = np.zeros(nr_states)\n",
    "s0[0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Numpy version\n",
    "def np_markov_step(s_tm1): \n",
    "    s_t = np.dot(s_tm1, A.T)\n",
    "    return s_t \n",
    "\n",
    "def np_markov_chain(nr_steps, A, s0):\n",
    "    # Pre-allocate space\n",
    "    s = np.zeros((nr_steps+1, nr_states))\n",
    "    s[0, :] = s0\n",
    "    for t in np.arange(nr_steps):\n",
    "        s[t+1, :] = np_markov_step(s[t, :])\n",
    "    return  s           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.        ,  0.        ],\n",
       "       [ 0.36331926,  0.00998989,  0.62669084],\n",
       "       [ 0.22093512,  0.07267485,  0.70639002],\n",
       "       [ 0.22723631,  0.08820146,  0.68456223],\n",
       "       [ 0.23850484,  0.08797088,  0.67352428],\n",
       "       [ 0.24099095,  0.08686008,  0.67214897]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_markov_chain(nr_steps, A, s0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Theano version\n",
    "# Store variables as shared variables\n",
    "th_A = theano.shared(A, name='A', borrow=True)\n",
    "th_s0 = theano.shared(s0, name='s0', borrow=True)\n",
    "# Symbolic variable for the number of steps\n",
    "th_nr_steps = T.lscalar('nr_steps')\n",
    "\n",
    "def th_markov_step(s_tm1): \n",
    "    s_t = T.dot(s_tm1, th_A.T)\n",
    "    # Remember to name variables\n",
    "    s_t.name = 's_t'\n",
    "    return s_t \n",
    "\n",
    "s, _ = theano.scan(th_markov_step, \n",
    "                   outputs_info=[dict(initial=th_s0)], \n",
    "                   n_steps=th_nr_steps)\n",
    "th_markov_chain = theano.function([th_nr_steps], T.concatenate((th_s0[None, :], s), 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.        ,  0.        ],\n",
       "       [ 0.36331926,  0.00998989,  0.62669084],\n",
       "       [ 0.22093512,  0.07267485,  0.70639002],\n",
       "       [ 0.22723631,  0.08820146,  0.68456223],\n",
       "       [ 0.23850484,  0.08797088,  0.67352428],\n",
       "       [ 0.24099095,  0.08686008,  0.67214897]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "th_markov_chain(nr_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6.3\n",
    "Complete the theano code for a RNN inside lxmls/deep learning/rnn.py. Use exercise 6.1 for a numpy example and 6.2 to learn how to handle scan. Keep in mind that you only need to implement the forward pass!. Theano will handle backpropagation ofr us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rnn = rnns.RNN(nr_words, emb_size, hidden_size, nr_tags, seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compile theano function\n",
    "x = T.ivector('x')\n",
    "th_forward = theano.function([x], rnn._forward(x).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with theano, it is more difficult to localize the source of errors. It is therefore extremely important to work step by step and test the code frequently. To debug we suggest to implement and compile the forward pass first. You can use this code for testing. If it raises no error you are good to go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert np.allclose(th_forward(x0), np_rnn.forward(x0)), \\\n",
    "    \"Numpy and Theano forward pass differ!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you are confident the forward pass is working you can test the gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compile function returning the list of gradients\n",
    "x = T.ivector('x')     # Input words\n",
    "y = T.ivector('y')     # gold tags \n",
    "p_y = rnn._forward(x)\n",
    "cost = -T.mean(T.log(p_y)[T.arange(y.shape[0]), y])\n",
    "grads_fun = theano.function([x, y], [T.grad(cost, par) for par in rnn.param])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compare numpy and theano gradients\n",
    "theano_rnn_gradients = grads_fun(x0, y0)\n",
    "for n in range(len(theano_rnn_gradients)): \n",
    "    assert np.allclose(numpy_rnn_gradients[n], theano_rnn_gradients[n]), \\\n",
    "    \"Numpy and Theano gradients differ in step n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, its time to test our network!. For this, lets first compile a function that does predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rnn_prediction = theano.function([x], T.argmax(p_y, 1))\n",
    "# Lets test the predictions\n",
    "def test_model(sample_seq, rnn_prediction):\n",
    "    words = [train_seq.word_dict[wrd] for wrd in sample_seq.x]\n",
    "    tags = [train_seq.tag_dict[pred] for pred in rnn_prediction(sample_seq.x)]\n",
    "    print [\"/\".join([word, tag]) for word , tag in zip(words, tags)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'Ms./noun', u'Haag/noun', u'plays/noun', u'Elianti/noun', u'./noun']\n"
     ]
    }
   ],
   "source": [
    "test_model(train_seq[0], rnn_prediction) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets define the optimization parameters and compile a batch update function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lrate = 0.5\n",
    "n_iter = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get list of SGD batch update rule for each parameter\n",
    "updates = [(par, par - lrate*T.grad(cost, par)) for par in rnn.param]\n",
    "# compile\n",
    "rnn_batch_update = theano.function([x, y], cost, updates=updates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally it is time to run SGD. You can use the following code for this purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train cost 2305.56 Acc 39.76 %  Devel Acc 82.51 %\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-6e248dbea0c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mcost\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mrnn_batch_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0merrors\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0macc_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnr_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Epoch %d: Train cost %2.2f Acc %2.2f %%\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'position_of_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/theano/scan_module/scan_op.pyc\u001b[0m in \u001b[0;36mrval\u001b[0;34m(p, i, o, n, allow_gc)\u001b[0m\n\u001b[1;32m    949\u001b[0m         def rval(p=p, i=node_input_storage, o=node_output_storage, n=node,\n\u001b[1;32m    950\u001b[0m                  allow_gc=allow_gc):\n\u001b[0;32m--> 951\u001b[0;31m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    952\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m                 \u001b[0mcompute_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/theano/scan_module/scan_op.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(node, args, outs)\u001b[0m\n\u001b[1;32m    938\u001b[0m                         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m                         \u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 940\u001b[0;31m                         self, node)\n\u001b[0m\u001b[1;32m    941\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mImportError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgof\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMissingGXX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mtheano/scan_module/scan_perform.pyx\u001b[0m in \u001b[0;36mtheano.scan_module.scan_perform.perform (/Users/ramon/.theano/compiledir_Darwin-13.4.0-x86_64-i386-64bit-i386-2.7.10-64/scan_perform/mod.cpp:4193)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/theano/gof/op.pyc\u001b[0m in \u001b[0;36mrval\u001b[0;34m(p, i, o, n)\u001b[0m\n\u001b[1;32m    910\u001b[0m             \u001b[0;31m# default arguments are stored in the closure of `rval`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m                 \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m                     \u001b[0mcompute_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/theano/tensor/basic.pyc\u001b[0m in \u001b[0;36mperform\u001b[0;34m(self, node, inp, out)\u001b[0m\n\u001b[1;32m   5434\u001b[0m         \u001b[0;31m# gives a numpy float object but we need to return a 0d\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5435\u001b[0m         \u001b[0;31m# ndarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5436\u001b[0;31m         \u001b[0mz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5438\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nr_words = sum([len(seq.x) for seq in train_seq])\n",
    "for i in range(n_iter):\n",
    "    \n",
    "    # Training\n",
    "    cost = 0\n",
    "    errors = 0\n",
    "    for n, seq in enumerate(train_seq):\n",
    "        cost += rnn_batch_update(seq.x, seq.y)\n",
    "        errors += sum(rnn_prediction(seq.x) != seq.y)\n",
    "    acc_train = 100*(1-errors*1./nr_words) \n",
    "    print \"Epoch %d: Train cost %2.2f Acc %2.2f %%\" % (i+1, cost, acc_train), \n",
    "    \n",
    "    # Evaluation    \n",
    "    errors = 0\n",
    "    for n, seq in enumerate(dev_seq):\n",
    "        errors += sum(rnn_prediction(seq.x) != seq.y)  \n",
    "    acc_dev = 100*(1-errors*1./nr_words) \n",
    "    print \" Devel Acc %2.2f %%\" % acc_dev\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the effect of using pre-trained embeddings. Run the following code to download the embeddings, reset the layer parameters and initialize the embedding layer with the pre-trained embeddings. Then run the training code above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting embeddings for the vocabulary 3618/4786 \n",
      "24.4% missing embeddings, set to random\n"
     ]
    }
   ],
   "source": [
    "# Embeddings Path\n",
    "EMBEDDINGS = \"../../data/senna_50\"\n",
    "import lxmls.deep_learning.embeddings as emb\n",
    "import os\n",
    "reload(emb)\n",
    "if not os.path.isfile(EMBEDDINGS):\n",
    "    emb.download_embeddings('senna_50', EMBEDDINGS)\n",
    "E = emb.extract_embeddings(EMBEDDINGS, train_seq.x_dict) \n",
    "# Reset model to remove the effect of training\n",
    "rnn = rnns.reset_model(rnn, seed=SEED)\n",
    "# Set the embedding layer to the pre-trained values\n",
    "rnn.param[0].set_value(E.astype(theano.config.floatX)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Exercise 6.4\n",
    "Convince yourself that LSTMs are just more complex RNNs. Run them, play around with the hyper parameters and compare the RNN and LSTM classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lstm = rnns.LSTM(nr_words, emb_size, hidden_size, nr_tags)\n",
    "lstm_prediction = theano.function([x], \n",
    "                                  T.argmax(lstm._forward(x), 1))\n",
    "lstm_cost = -T.mean(T.log(lstm._forward(x))[T.arange(y.shape[0]), y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get list of SGD batch update rule for each parameter\n",
    "lstm_updates = [(par, par - lrate*T.grad(lstm_cost, par)) for par in lstm.param]\n",
    "# compile\n",
    "lstm_batch_update = theano.function([x, y], lstm_cost, \n",
    "                                    updates=lstm_updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nr_words = sum([len(seq.x) for seq in train_seq])\n",
    "for i in range(n_iter):\n",
    "    \n",
    "    # Training\n",
    "    cost = 0\n",
    "    errors = 0\n",
    "    for n, seq in enumerate(train_seq):\n",
    "        cost += lstm_batch_update(seq.x, seq.y)\n",
    "        errors += sum(lstm_prediction(seq.x) != seq.y)\n",
    "    acc_train = 100*(1-errors*1./nr_words) \n",
    "    print \"Epoch %d: Train cost %2.2f Acc %2.2f %%\" % (i+1, cost, acc_train), \n",
    "    \n",
    "    # Evaluation    \n",
    "    errors = 0\n",
    "    for n, seq in enumerate(dev_seq):\n",
    "        errors += sum(lstm_prediction(seq.x) != seq.y)  \n",
    "    acc_dev = 100*(1-errors*1./nr_words) \n",
    "    print \" Devel Acc %2.2f %%\" % acc_dev\n",
    "    sys.stdout.flush()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
