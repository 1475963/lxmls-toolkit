{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amazon Sentiment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import lxmls.readers.sentiment_reader as srs\n",
    "from lxmls.deep_learning.utils import AmazonData\n",
    "corpus = srs.SentimentCorpus(\"books\")\n",
    "data = AmazonData(corpus=corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2 Implement Backpropagation for an MLP in Numpy and train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "geometry = [corpus.nr_features, 20, 2]\n",
    "activation_functions = ['sigmoid', 'softmax']\n",
    "\n",
    "# Optimization\n",
    "learning_rate = 0.05\n",
    "num_epochs = 10\n",
    "batch_size = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxmls.deep_learning.numpy_models.mlp import NumpyMLP\n",
    "model = NumpyMLP(\n",
    "    geometry=geometry,\n",
    "    activation_functions=activation_functions,\n",
    "    learning_rate=learning_rate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Milestone 1:\n",
    "Check gradients using the empirical gradient computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the loss values for the study weight and perturbed versions. Take into account that the gradient for the first layer (embeddings) will be zero unless the word is in the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxmls.deep_learning.mlp import get_mlp_parameter_handlers, get_mlp_loss_range\n",
    "\n",
    "# Get functions to get and set values of a particular weight of the model\n",
    "get_parameter, set_parameter = get_mlp_parameter_handlers(\n",
    "    layer_index=1,\n",
    "    is_bias=False,\n",
    "    row=0, \n",
    "    column=0\n",
    ")\n",
    "\n",
    "# Get batch of data\n",
    "batch = data.batches('train', batch_size=batch_size)[0]\n",
    "\n",
    "# Get loss and weight value\n",
    "current_loss = model.cross_entropy_loss(batch['input'], batch['output'])\n",
    "current_weight = get_parameter(model.parameters)\n",
    "\n",
    "# Get range of values of the weight and loss around current parameters values\n",
    "weight_range, loss_range = get_mlp_loss_range(model, get_parameter, set_parameter, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this to know the non-zero values of the input (that have non-zero gradient)\n",
    "batch['input'][0].nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the gradient value for that weight\n",
    "current_gradient = get_parameter((model.backpropagation(batch['input'], batch['output'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "# Plot empirical\n",
    "plt.plot(weight_range, loss_range)\n",
    "plt.plot(current_weight, current_loss, 'xr')\n",
    "plt.ylabel('loss value')\n",
    "plt.xlabel('weight value')\n",
    "# Plot real\n",
    "h = plt.plot(\n",
    "    weight_range,\n",
    "    current_gradient*(weight_range - current_weight) + current_loss, \n",
    "    'r--'\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Milestone 2:\n",
    "Train a MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get batch iterators for train and test\n",
    "train_batches = data.batches('train', batch_size=batch_size)\n",
    "test_set = data.batches('test', batch_size=None)[0]\n",
    "\n",
    "# Epoch loop\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # Batch loop\n",
    "    for batch in train_batches:\n",
    "        model.update(input=batch['input'], output=batch['output'])\n",
    "\n",
    "    # Prediction for this epoch\n",
    "    hat_y = model.predict(input=test_set['input'])\n",
    "\n",
    "    # Evaluation\n",
    "    accuracy = 100*np.mean(hat_y == test_set['output'])\n",
    "\n",
    "    # Inform user\n",
    "    print(\"Epoch %d: accuracy %2.2f %%\" % (epoch+1, accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
